1) Using Complex datatypes in HQL:
	a.Array
	b.Map
	c.Struct

2) View

3) User Defined functions (UDF)

Using Arrays in HQL query, explaining with the example shared by sir:
Step 1:In the flat file arrayfile, the 4th column of each row holds data to be copied to an array.
	a$b$c: here each array element is separated by '$'.
Step 2: Load this data into a hive table. While creating the column for the 4th element, declare the type of the column as 'array'. Also mention the delimited for the array as '$'
step 3:To access the array elements from the table in a HQL query, access the array elements with [] and index.

sample data:
ARRAY:

$ cat >arrayfile
1,abc,40000,a$b$c,hyd
2,def,3000,d$f,bang
//LAB Start
hive>
create database complex;
use complex;

create table tab7(id int,name string,sal bigint,sub array<string>,city string) row format delimited fields terminated by ',' collection items terminated by '$';

load data local inpath '/home/hduser/arrayfile' overwrite into table tab7;

select * from tab7;

select * from tab7;

//Accessing array element using index
select id, name, sal, sub[2] from tab7;

//Accessing array using values, when index is not known(get all the elements from tab7 which has  an array value of 'b'))
select * from tab7 where array_contains(sub,'b');
//LAB End


Using Map:
Step 1: In the  flat file mapfile,the 5th value is for map entry. Each map entry is separated by '$'. The Key and value pairs are separated by #. 
Step 2: While loading the mapfile data to hive table,define the type as map and also specify the delimiter for key and value and for row in the map.
step 3: To access the mapvalue in the table from a HQL query, use columnName["keyname"]. 


$ cat >mapfile
1,abc,40000,a$b$c,bonus#500$insurance#200,city1
2,def,3000,d$f,bonus#500,city2
3,xyz,10000,g$h,esic#1000$da#400,city2

//LAB Start
hive>
create table tab10(id int,name string,sal bigint,sub array<string>,dud map<string,int>,city string)
row format delimited 
fields terminated by ','
collection items terminated by '$'
map keys terminated by '#';

load data local inpath '/home/hduser/mapfile' overwrite into table tab10;

hive>select * from tab10;

hive>select dud["bonus"] from tab10; 

hive>select dud["bonus"],dud["insurance"] from tab10; 
//LAB End

Struct:
Used to store data of different datatype in one column. Eg. store address with string, number and pincode all one column.
In the below flat file structfile, the 6th colmun element is has struct values separated by '$'
cat >structfile
1,abc,40000,a$b$c,bonus#500$insurance#200,city1$state1$111
2,def,3000,d$f,bonus#500,city2$state2$222

LAB Start
hive>
create table tab11(id int,name string,sal bigint,sub array<string>,dud map<string,int>,addr struct<city:string,state:string,pin:bigint>)
row format delimited 
fields terminated by ','
collection items terminated by '$'
map keys terminated by '#';

load data local inpath '/home/hduser/structfile' into table tab11;

hive>select addr.city from tab11;
LAB End

View:
Views are virtual table. A view is associated toa query and saved.
Later when the saved view is run, the query gets executed.

hive> select * from emprecords;
+------+--------------+-------------+-------------------+--------+
| ID   | Name         | Salary      | Designation       | Dept   |
+------+--------------+-------------+-------------------+--------+
|1201  | Gopal        | 45000       | Technical manager | TP     |
|1202  | Manisha      | 45000       | Proofreader       | PR     |
|1203  | Masthanvali  | 40000       | Technical writer  | TP     |
|1204  | Krian        | 40000       | Hr Admin          | HR     |
|1205  | Kranthi      | 30000       | Op Admin          | Admin  |
+------+--------------+-------------+-------------------+--------+

//Create a view named emp_30000
hive>CREATE VIEW emp_30000 AS
SELECT * FROM employee
WHERE salary>30000;

//Executing the saved view emp_30000
hive> select* from emp_30000 order by salary desc;



User Defined Function:
Step 1: Create java class extending from UDF.
Step 2: Import external jar files hive-exec-1.2.1.jar, hadoop-common-2.6.0.jar (refer the images I sent in whatsapp) 
Step 3: Export the jar file.
Step 4: Add the jar file to the hdfs.
Step 5: Create a temporary function through which the java function will be accessed in the HQL Query. (Create temporary function UnixTime as '<packagename.classname>')
Step 6: Execute the query.

Lab Start:
Step 1: On eclipse, select your working project. Create a package udfhive. Import all the .java files sent by sir today. Eg. UnixTimeToData.java
Import the external jar.
Step 2: Now the code will compile. Export the jar file
Step 3: hive> add jar /home/hduser/....UnixTimeToDate.jar;
        hive> list jars;
step 4: hive> create table unixtablerecs(id string, unixtime string) row format delimited fields terminated by ',';
step 5: hive> load data local inpath '/home/hduser/counter.txt' into table unixtablerecs;
step 6: hive> Create temporary function UnixTime as 'udfhive.UnixtimeToDate';
	hive> show functions;
step 7: hive> select id, UnixTime(unixtime) from unixtablerecs;

Similary use the oter two java function to get word count and convert to lowercase.

LAB End

	


